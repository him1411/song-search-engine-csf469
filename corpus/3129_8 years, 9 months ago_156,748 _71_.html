I\'m using the subprocess module to start a subprocess and connect to it\'s output stream (stdout). I want to be able to execute non-blocking reads on its stdout. Is there a way to make .readline non-blocking or to check if there is data on the stream before I invoke .readline? I\'d like this to be portable or at least work under Windows and Linux.here is how I do it for now (It\'s blocking on the .readline if no data is avaible):fcntl, select, asyncproc won\'t help in this case.A reliable way to read a stream without blocking regardless of operating system is to use Queue.get_nowait():I have often had a similar problem; Python programs I write frequently need to have the ability to execute some primary functionality while simultaneously accepting user input from the command line (stdin). Simply putting the user input handling functionality in another thread doesn\'t solve the problem because readline() blocks and has no timeout. If the primary functionality is complete and there is no longer any need to wait for further user input I typically want my program to exit, but it can\'t because readline() is still blocking in the other thread waiting for a line. A solution I have found to this problem is to make stdin a non-blocking file using the fcntl module:In my opinion this is a bit cleaner than using the select or signal modules to solve this problem but then again it only works on UNIX...Python 3.4 introduces new provisional API for asynchronous IO -- asyncio module. The approach is similar to twisted-based answer by @Bryan Ward -- define a protocol and its methods are called as soon as data is ready:See "Subprocess" in the docs.There is a high-level interface asyncio.create_subprocess_exec() that returns Process objects that allows to read a line asynchroniosly using StreamReader.readline() coroutine \n(with async/await Python 3.5+ syntax):readline_and_kill() performs the following tasks:Each step could be limited by timeout seconds if necessary.Try the asyncproc module. For example:The module takes care of all the threading as suggested by S.Lott.You can do this really easily in Twisted. Depending upon your existing code base, this might not be that easy to use, but if you are building a twisted application, then things like this become almost trivial. You create a ProcessProtocol class, and override the outReceived() method. Twisted (depending upon the reactor used) is usually just a big select() loop with callbacks installed to handle data from different file descriptors (often network sockets). So the outReceived() method is simply installing a callback for handling data coming from STDOUT.  A simple example demonstrating this behavior is as follows:The Twisted documentation has some good information on this.If you build your entire application around Twisted, it makes asynchronous communication with other processes, local or remote, really elegant like this. On the other hand, if your program isn\'t built on top of Twisted, this isn\'t really going to be that helpful. Hopefully this can be helpful to other readers, even if it isn\'t applicable for your particular application.Use select & read(1).  For readline()-like:One solution is to make another process to perform your read of the process, or make a thread of the process with a timeout.Here\'s the threaded version of a timeout function:http://code.activestate.com/recipes/473878/However, do you need to read the stdout as it\'s coming in?\nAnother solution may be to dump the output to a file and wait for the process to finish using p.wait().Disclaimer: this works only for tornadoYou can do this by setting the fd to be nonblocking and then use ioloop to register callbacks. I have packaged this in an egg called tornado_subprocess and you can install it via PyPI:now you can do something like this:you can also use it with a RequestHandlerExisting solutions did not work for me (details below). What finally worked was to implement readline using read(1) (based on this answer). The latter does not block:Why existing solutions did not work:I add this problem to read some subprocess.Popen stdout.\nHere is my non blocking read solution:Here is my code, used to catch every output from subprocess ASAP, including partial lines. It pumps at same time and stdout and stderr in almost correct order.Tested and correctly worked on Python 2.7 linux & windows.\nThis version of non-blocking read doesn\'t require special modules and will work out-of-the-box on majority of Linux distros.The select module helps you determine where the next useful input is.However, you\'re almost always happier with separate threads.  One does a blocking read the stdin, another does wherever it is you don\'t want blocked.Adding this answer here since it provides ability to set non-blocking pipes on Windows and Unix.All the ctypes details are thanks to @techtonik\'s answer.There is a slightly modified version to be used both on Unix and Windows systems.This way you can use the same function and exception for Unix and Windows code.To avoid reading incomplete data, I ended up writing my own readline generator (which returns the byte string for each line).Its a generator so you can for example...I have created a library based on J. F. Sebastian\'s solution. You can use it.https://github.com/cenkalti/whatWorking from J.F. Sebastian\'s answer, and several other sources, I\'ve put together a simple subprocess manager. It provides the request non-blocking reading, as well as running several processes in parallel. It doesn\'t use any OS-specific call (that I\'m aware) and thus should work anywhere.It\'s available from pypi, so just pip install shelljob. Refer to the project page for examples and full docs.EDIT: This implementation still blocks. Use J.F.Sebastian\'s answer instead.I tried the top answer, but the additional risk and maintenance of thread code was worrisome.Looking through the io module (and being limited to 2.6), I found BufferedReader. This is my threadless, non-blocking solution.\nI recently stumbled upon on the same problem\nI need to read one line at time from stream ( tail run in subprocess )\nin non-blocking mode \nI wanted to avoid next problems: not to burn cpu, don\'t read stream by one byte (like readline did ), etcHere is my implementation\nhttps://gist.github.com/grubberr/5501e1a9760c3eab5e0a\nit don\'t support windows (poll), don\'t handle EOF,\nbut it works for me wellwhy bothering thread&queue?\nunlike readline(), BufferedReader.read1() wont block waiting for \\r\\n, it returns ASAP if there is any output coming in.In my case I needed a logging module that catches the output from the background applications and augments it(adding time-stamps, colors, etc.).I ended up with a background thread that does the actual I/O. Following code is only for POSIX platforms. I stripped non-essential parts. If someone is going to use this beast for long runs consider managing open descriptors. In my case it was not a big problem.This is a example to run interactive command in subprocess, and the stdout is interactive by using pseudo terminal. You can refer to: https://stackoverflow.com/a/43012138/3555925My problem is a bit different as I wanted to collect both stdout and stderr from a running process, but ultimately the same since I wanted to render the output in a widget as its generated.I did not want to resort to many of the proposed workarounds using Queues or additional Threads as they should not be necessary to perform such a common task as running another script and collecting its output.After reading the proposed solutions and python docs I resolved my issue with the implementation below. Yes it only works for POSIX as I\'m using the select function call. I agree that the docs are confusing and the implementation is awkward for such a common scripting task. I believe that older versions of python have different defaults for Popen and different explanations so that created a lot of confusion. This seems to work well for both Python 2.7.12 and 3.5.2. The key was to set bufsize=1 for line buffering and then universal_newlines=True to process as a text file instead of a binary which seems to become the default when setting bufsize=1.ERROR, DEBUG and VERBOSE are simply macros that print output to the terminal.This solution is IMHO 99.99% effective as it still uses the blocking readline function, so we assume the sub process is nice and outputs complete lines. I welcome feedback to improve the solution as I am still new to Python.Here is a module that supports non-blocking reads and background writes in python:https://pypi.python.org/pypi/python-nonblockProvides a function,nonblock_read which will read data from the stream, if available, otherwise return an empty string (or None if the stream is closed on the other side and all possible data has been read)You may also consider the python-subprocess2 module,https://pypi.python.org/pypi/python-subprocess2which adds to the subprocess module. So on the object returned from "subprocess.Popen" is added an additional method, runInBackground. This starts a thread and returns an object which will automatically be populated as stuff is written to stdout/stderr, without blocking your main thread.Enjoy!